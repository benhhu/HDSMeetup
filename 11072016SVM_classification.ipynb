{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PIMA Indians Diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataset = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "preg       int64\n",
      "plas       int64\n",
      "pres       int64\n",
      "skin       int64\n",
      "test       int64\n",
      "mass     float64\n",
      "pedi     float64\n",
      "age        int64\n",
      "class      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0      6   148    72    35     0  33.6  0.627   50      1\n",
      "1      1    85    66    29     0  26.6  0.351   31      0\n",
      "2      8   183    64     0     0  23.3  0.672   32      1\n",
      "3      1    89    66    23    94  28.1  0.167   21      0\n",
      "4      0   137    40    35   168  43.1  2.288   33      1\n",
      "5      5   116    74     0     0  25.6  0.201   30      0\n",
      "6      3    78    50    32    88  31.0  0.248   26      1\n",
      "7     10   115     0     0     0  35.3  0.134   29      0\n",
      "8      2   197    70    45   543  30.5  0.158   53      1\n",
      "9      8   125    96     0     0   0.0  0.232   54      1\n",
      "10     4   110    92     0     0  37.6  0.191   30      0\n",
      "11    10   168    74     0     0  38.0  0.537   34      1\n",
      "12    10   139    80     0     0  27.1  1.441   57      0\n",
      "13     1   189    60    23   846  30.1  0.398   59      1\n",
      "14     5   166    72    19   175  25.8  0.587   51      1\n",
      "15     7   100     0     0     0  30.0  0.484   32      1\n",
      "16     0   118    84    47   230  45.8  0.551   31      1\n",
      "17     7   107    74     0     0  29.6  0.254   31      1\n",
      "18     1   103    30    38    83  43.3  0.183   33      0\n",
      "19     1   115    70    30    96  34.6  0.529   32      1\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = dataset.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVM with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 10**np.arange(0,5)\n",
    "param_grid = dict(C=C)\n",
    "model = SVC(random_state=seed, probability=True, kernel=\"linear\")\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=1)\n",
    "grid.fit(rescaledX_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=7, shrinking=True, tol=0.001,\n",
       "  verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(random_state=seed, probability=True, kernel=\"linear\",C=1)\n",
    "model.fit(rescaledX_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 10**np.arange(0,5)\n",
    "kernel = ['rbf']\n",
    "#degree = np.arange(1,5)\n",
    "gamma = 10**np.arange(0,10)\n",
    "\n",
    "param_grid = dict(C=C,kernel=kernel,gamma=gamma)\n",
    "model = SVC(random_state=seed, probability=True)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=-1)\n",
    "grid.fit(rescaledX_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788322105318\n",
      "(5, 9, 2, 1.0000000000000001e-05)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.max_depth, grid.best_estimator_.min_samples_leaf, grid.best_estimator_.min_samples_split, grid.best_estimator_.min_impurity_split)\n",
    "dtTunedMaxDepth = grid.best_estimator_.max_depth\n",
    "dtTunedMinSamplesLeaf = grid.best_estimator_.min_samples_leaf\n",
    "dtTunedMinSamplesSplit = grid.best_estimator_.min_samples_split\n",
    "dtTunedMinImpuritySplit = grid.best_estimator_.min_impurity_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bagged Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1.0000000000000001e-05, min_samples_leaf=9,\n",
       "            min_samples_split=2, min_weight_fraction_leaf...0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=7, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': array([100, 200, 300, 400]), 'max_samples': array([ 1. ,  0.9,  0.8,  0.7])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators  = np.arange(100,500,100)\n",
    "max_samples = np.arange(1.0,0.7,-0.1)\n",
    "\n",
    "param_grid = dict(n_estimators =n_estimators ,max_samples=max_samples)\n",
    "bdt_dt = DecisionTreeClassifier(random_state=seed, max_depth=dtTunedMaxDepth,min_samples_leaf=dtTunedMinSamplesLeaf,min_samples_split=dtTunedMinSamplesSplit,min_impurity_split=dtTunedMinImpuritySplit)\n",
    "model = BaggingClassifier(base_estimator=bdt_dt, random_state=seed)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822039297929\n",
      "(400, 0.70000000000000007)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_estimators, grid.best_estimator_.max_samples)\n",
    "bdtTunedNEstimators = grid.best_estimator_.n_estimators\n",
    "bdtTunedMaxSamples = grid.best_estimator_.max_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=7,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': array([ 0.8,  0.7,  0.6]), 'min_impurity_split': array([  1.00000e-07,   1.00000e-08]), 'n_estimators': array([ 50,  75, 100, 125]), 'max_depth': array([2, 3, 4]), 'min_samples_leaf': array([1, 3, 5])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators  = np.arange(50,150,25)\n",
    "max_features = np.arange(0.8,0.6,-0.1)\n",
    "max_depth = np.arange(2,5)\n",
    "min_samples_leaf = np.arange(1,6,2)\n",
    "min_impurity_split = 10.0**-np.arange(7,9)\n",
    "\n",
    "param_grid = dict(n_estimators=n_estimators,\n",
    "                  max_features=max_features, \n",
    "                  max_depth=max_depth, \n",
    "                  min_samples_leaf=min_samples_leaf,\n",
    "                  min_impurity_split=min_impurity_split)\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827291561066\n",
      "(125, 0.70000000000000007, 4, 5, 9.9999999999999995e-08)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_estimators, \n",
    "      grid.best_estimator_.max_features,\n",
    "      grid.best_estimator_.max_depth,\n",
    "      grid.best_estimator_.min_samples_leaf,\n",
    "      grid.best_estimator_.min_impurity_split)\n",
    "rfTunedNEstimators = grid.best_estimator_.n_estimators\n",
    "rfTunedMaxFeatures = grid.best_estimator_.max_features\n",
    "rfTunedMaxDepth = grid.best_estimator_.max_depth\n",
    "rfTunedMinSamplesLeaf = grid.best_estimator_.min_samples_leaf\n",
    "rfTunedMinImpuritySplit = grid.best_estimator_.min_impurity_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=7,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': array([ 0.8,  0.7,  0.6]), 'min_impurity_split': array([  1.00000e-07,   1.00000e-08]), 'n_estimators': array([ 50,  75, 100, 125]), 'max_depth': array([2, 3, 4]), 'min_samples_leaf': array([1, 3, 5])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators  = np.arange(50,150,25)\n",
    "max_features = np.arange(0.8,0.6,-0.1)\n",
    "max_depth = np.arange(2,5)\n",
    "min_samples_leaf = np.arange(1,6,2)\n",
    "min_impurity_split = 10.0**-np.arange(7,9)\n",
    "\n",
    "param_grid = dict(n_estimators=n_estimators,\n",
    "                  max_features=max_features, \n",
    "                  max_depth=max_depth, \n",
    "                  min_samples_leaf=min_samples_leaf,\n",
    "                  min_impurity_split=min_impurity_split)\n",
    "model = ExtraTreesClassifier(random_state=seed)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829275263608\n",
      "(100, 0.70000000000000007, 3, 3, 9.9999999999999995e-08)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_estimators, \n",
    "      grid.best_estimator_.max_features,\n",
    "      grid.best_estimator_.max_depth,\n",
    "      grid.best_estimator_.min_samples_leaf,\n",
    "      grid.best_estimator_.min_impurity_split)\n",
    "etTunedNEstimators = grid.best_estimator_.n_estimators\n",
    "etTunedMaxFeatures = grid.best_estimator_.max_features\n",
    "etTunedMaxDepth = grid.best_estimator_.max_depth\n",
    "etTunedMinSamplesLeaf = grid.best_estimator_.min_samples_leaf\n",
    "etTunedMinImpuritySplit = grid.best_estimator_.min_impurity_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=7, splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=7),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': array([ 50,  75, 100, 125]), 'learning_rate': array([ 1.  ,  1.25,  1.5 ,  1.75]), 'base_estimator__max_depth': array([2, 3, 4]), 'base_estimator__min_samples_leaf': array([1, 2, 3, 4, 5]), 'base_estimator__min_impurity_split': array([  1.00000e-07,   1.00000e-08])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators  = np.arange(50,150,25)\n",
    "learning_rate = np.arange(1,2,0.25)\n",
    "base_estimator__max_depth = np.arange(2,5)\n",
    "base_estimator__min_samples_leaf = np.arange(1,6,1)\n",
    "base_estimator__min_impurity_split = 10.0**-np.arange(7,9)\n",
    "\n",
    "param_grid = dict(n_estimators =n_estimators,\n",
    "                  learning_rate=learning_rate,\n",
    "                  base_estimator__max_depth=base_estimator__max_depth,\n",
    "                  base_estimator__min_samples_leaf=base_estimator__min_samples_leaf,\n",
    "                  base_estimator__min_impurity_split=base_estimator__min_impurity_split)\n",
    "adaboost_dt = DecisionTreeClassifier(random_state=seed)\n",
    "model = AdaBoostClassifier(base_estimator=adaboost_dt, random_state=seed)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803902152719\n",
      "(125, 1.25, 4, 1, 9.9999999999999995e-08)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_estimators, \n",
    "      grid.best_estimator_.learning_rate,\n",
    "      grid.best_estimator_.base_estimator.max_depth,\n",
    "      grid.best_estimator_.base_estimator.min_samples_leaf,\n",
    "      grid.best_estimator_.base_estimator.min_impurity_split)\n",
    "adaboostTunedNEstimators = grid.best_estimator_.n_estimators\n",
    "adaboostTunedLearningRate = grid.best_estimator_.learning_rate\n",
    "adaboostTunedMaxDepth = grid.best_estimator_.base_estimator.max_depth\n",
    "adaboostTunedMinSamplesLeaf = grid.best_estimator_.base_estimator.min_samples_leaf\n",
    "adaboostTunedMinImpuritySplit = grid.best_estimator_.base_estimator.min_impurity_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=7,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': array([100, 300]), 'min_impurity_split': array([  1.00000e-07,   1.00000e-08]), 'learning_rate': array([ 0.1  ,  0.01 ,  0.001]), 'max_depth': array([2, 3, 4]), 'min_samples_leaf': array([1, 2, 3, 4, 5])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators  = np.arange(100,500,200)\n",
    "learning_rate = 10.0**-np.arange(1,4)\n",
    "max_depth = np.arange(2,5)\n",
    "min_samples_leaf = np.arange(1,6,1)\n",
    "min_impurity_split = 10.0**-np.arange(7,9)\n",
    "\n",
    "param_grid = dict(n_estimators =n_estimators,\n",
    "                  learning_rate=learning_rate,\n",
    "                  max_depth=max_depth,\n",
    "                  min_samples_leaf=min_samples_leaf,\n",
    "                  min_impurity_split=min_impurity_split)\n",
    "model = GradientBoostingClassifier(random_state=seed)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 4\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, param_grid=param_grid,scoring=scoring,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818444742544\n",
      "(300, 0.01, 3, 1, 9.9999999999999995e-08)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_estimators, \n",
    "      grid.best_estimator_.learning_rate,\n",
    "      grid.best_estimator_.max_depth,\n",
    "      grid.best_estimator_.min_samples_leaf,\n",
    "      grid.best_estimator_.min_impurity_split)\n",
    "sgbTunedNEstimators = grid.best_estimator_.n_estimators\n",
    "sgbTunedLearningRate = grid.best_estimator_.learning_rate\n",
    "sgbTunedMaxDepth = grid.best_estimator_.max_depth\n",
    "sgbTunedMinSamplesLeaf = grid.best_estimator_.min_samples_leaf\n",
    "sgbTunedMinImpuritySplit = grid.best_estimator_.min_impurity_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Implementation of Stochastic Gradient Boosting: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=7, silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240]), 'subsample': array([ 1.  ,  0.95,  0.9 ,  0.85,  0.8 ]), 'learning_rate': array([ 0.03 ,  0.035,  0.04 ,  0.045,  0.05 ,  0.055]), 'colsample_bytree': array([ 1.  ,  0.95,  0.9 ,  0.85]), 'max_depth': array([2, 3, 4])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators  = np.arange(100,250,10)\n",
    "learning_rate = np.arange(0.03,0.06,0.005)\n",
    "max_depth = np.arange(2,5)\n",
    "subsample = np.arange(1.0,0.75,-0.05)\n",
    "colsample_bytree = np.arange(1.0,0.85,-0.05)\n",
    "\n",
    "param_grid = dict(n_estimators =n_estimators,\n",
    "                  learning_rate=learning_rate,\n",
    "                  max_depth=max_depth,\n",
    "                  subsample=subsample,\n",
    "                  colsample_bytree=colsample_bytree)\n",
    "model = XGBClassifier(seed=seed,\n",
    "                      objective = \"binary:logistic\",\n",
    "                      nthread = 1,\n",
    "                      silent = True)\n",
    "scoring = 'roc_auc' \n",
    "num_folds = 3\n",
    "grid = GridSearchCV(cv=num_folds, estimator=model, scoring=scoring, param_grid=param_grid,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837077432608\n",
      "(100, 0.054999999999999986, 2, 0.79999999999999982, 0.94999999999999996)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_estimators, \n",
    "      grid.best_estimator_.learning_rate,\n",
    "      grid.best_estimator_.max_depth,\n",
    "      grid.best_estimator_.subsample,\n",
    "      grid.best_estimator_.colsample_bytree)\n",
    "xgbTunedNEstimators = grid.best_estimator_.n_estimators\n",
    "xgbTunedLearningRate = grid.best_estimator_.learning_rate\n",
    "xgbTunedMaxDepth = grid.best_estimator_.max_depth\n",
    "xgbTunedSubsample = grid.best_estimator_.subsample\n",
    "xgbTunedColsampleBytree = grid.best_estimator_.colsample_bytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82539015685\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "\n",
    "model1 = BaggingClassifier(base_estimator=bdt_dt, random_state=seed, n_estimators=bdtTunedNEstimators, max_samples=bdtTunedMaxSamples)\n",
    "estimators.append(('bdt',model1))\n",
    "model2 = RandomForestClassifier(random_state=seed, n_estimators=rfTunedNEstimators, max_features=rfTunedMaxFeatures, max_depth=rfTunedMaxDepth, min_samples_leaf=rfTunedMinSamplesLeaf, min_impurity_split=rfTunedMinImpuritySplit)\n",
    "estimators.append(('rf',model2))\n",
    "#model3 = XGBClassifier(seed=seed,objective = \"binary:logistic\",nthread = 1,silent = True,n_estimators=xgbTunedNEstimators,learning_rate=xgbTunedLearningRate,max_depth=xgbTunedMaxDepth,subsample=xgbTunedSubsample,colsample_bytree=xgbTunedColsampleBytree)\n",
    "#estimators.append(('et',model3))\n",
    "\n",
    "voting = VotingClassifier(estimators,voting='soft')\n",
    "num_folds = 4\n",
    "results = cross_val_score(voting, X_train, Y_train, cv=num_folds, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "\n",
    "adaboost_dt = DecisionTreeClassifier(random_state=seed,\n",
    "                                     max_depth=adaboostTunedMaxDepth,\n",
    "                                     min_samples_leaf=adaboostTunedMinSamplesLeaf,\n",
    "                                     min_impurity_split=adaboostTunedMinImpuritySplit)\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('DT', Pipeline([('DT', DecisionTreeClassifier(random_state=seed, max_depth=dtTunedMaxDepth,min_samples_leaf=dtTunedMinSamplesLeaf,min_samples_split=dtTunedMinSamplesSplit,min_impurity_split=dtTunedMinImpuritySplit))])))\n",
    "pipelines.append(('BDT', Pipeline([('BDT', BaggingClassifier(base_estimator=bdt_dt, random_state=seed, n_estimators=bdtTunedNEstimators, max_samples=bdtTunedMaxSamples))])))\n",
    "pipelines.append(('RF', Pipeline([('RF', RandomForestClassifier(random_state=seed, n_estimators=rfTunedNEstimators, max_features=rfTunedMaxFeatures, max_depth=rfTunedMaxDepth, min_samples_leaf=rfTunedMinSamplesLeaf, min_impurity_split=rfTunedMinImpuritySplit))])))\n",
    "pipelines.append(('ET', Pipeline([('ET', ExtraTreesClassifier(random_state=seed, n_estimators=etTunedNEstimators, max_features=etTunedMaxFeatures, max_depth=etTunedMaxDepth, min_samples_leaf=etTunedMinSamplesLeaf, min_impurity_split=etTunedMinImpuritySplit))])))\n",
    "pipelines.append(('AdaBoost', Pipeline([('AdaBoost', AdaBoostClassifier(base_estimator=adaboost_dt, random_state=seed, n_estimators=adaboostTunedNEstimators, learning_rate=adaboostTunedLearningRate))])))\n",
    "pipelines.append(('SGB', Pipeline([('SGB', GradientBoostingClassifier(random_state=seed, n_estimators=sgbTunedNEstimators, learning_rate=sgbTunedLearningRate, max_depth=sgbTunedMaxDepth, min_samples_leaf=sgbTunedMinSamplesLeaf, min_impurity_split=sgbTunedMinImpuritySplit))])))\n",
    "pipelines.append(('XGB', Pipeline([('XGB', XGBClassifier(seed=seed,objective = \"binary:logistic\",nthread = 1,silent = True,n_estimators=xgbTunedNEstimators,learning_rate=xgbTunedLearningRate,max_depth=xgbTunedMaxDepth,subsample=xgbTunedSubsample,colsample_bytree=xgbTunedColsampleBytree))])))\n",
    "pipelines.append(('Voting', Pipeline([('Voting', voting)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: 0.798426\n",
      "BDT: 0.858383\n",
      "RF: 0.857298\n",
      "ET: 0.872671\n",
      "AdaBoost: 0.808826\n",
      "SGB: 0.843371\n",
      "XGB: 0.853681\n",
      "Voting: 0.859107\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test dataset\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    model.fit(X_train,Y_train)\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,model.predict_proba(X_test)[:,1])\n",
    "    result = auc(fpr,tpr)\n",
    "    results.append(result)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f\" % (name, result)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
